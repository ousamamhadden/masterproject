{
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 1.7523364485981308,
  "eval_steps": 250,
  "global_step": 3000,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.014602803738317757,
      "grad_norm": 1.453470230102539,
      "learning_rate": 4.97566199376947e-05,
      "loss": 1.3765,
      "step": 25
    },
    {
      "epoch": 0.029205607476635514,
      "grad_norm": 1.1255218982696533,
      "learning_rate": 4.9513239875389414e-05,
      "loss": 1.3412,
      "step": 50
    },
    {
      "epoch": 0.04380841121495327,
      "grad_norm": 1.6092039346694946,
      "learning_rate": 4.926985981308411e-05,
      "loss": 1.3306,
      "step": 75
    },
    {
      "epoch": 0.05841121495327103,
      "grad_norm": 1.085147738456726,
      "learning_rate": 4.902647975077882e-05,
      "loss": 1.3219,
      "step": 100
    },
    {
      "epoch": 0.07301401869158879,
      "grad_norm": 1.6311370134353638,
      "learning_rate": 4.8783099688473524e-05,
      "loss": 1.3352,
      "step": 125
    },
    {
      "epoch": 0.08761682242990654,
      "grad_norm": 0.695402204990387,
      "learning_rate": 4.853971962616823e-05,
      "loss": 1.3099,
      "step": 150
    },
    {
      "epoch": 0.1022196261682243,
      "grad_norm": 1.6968581676483154,
      "learning_rate": 4.829633956386293e-05,
      "loss": 1.3102,
      "step": 175
    },
    {
      "epoch": 0.11682242990654206,
      "grad_norm": 0.9237515926361084,
      "learning_rate": 4.8052959501557634e-05,
      "loss": 1.3168,
      "step": 200
    },
    {
      "epoch": 0.1314252336448598,
      "grad_norm": 1.2769345045089722,
      "learning_rate": 4.780957943925234e-05,
      "loss": 1.3098,
      "step": 225
    },
    {
      "epoch": 0.14602803738317757,
      "grad_norm": 1.3022220134735107,
      "learning_rate": 4.756619937694704e-05,
      "loss": 1.3056,
      "step": 250
    },
    {
      "epoch": 0.16063084112149534,
      "grad_norm": 1.5762336254119873,
      "learning_rate": 4.7322819314641744e-05,
      "loss": 1.3223,
      "step": 275
    },
    {
      "epoch": 0.17523364485981308,
      "grad_norm": 1.5681383609771729,
      "learning_rate": 4.707943925233645e-05,
      "loss": 1.3055,
      "step": 300
    },
    {
      "epoch": 0.18983644859813084,
      "grad_norm": 2.3322503566741943,
      "learning_rate": 4.6836059190031156e-05,
      "loss": 1.2932,
      "step": 325
    },
    {
      "epoch": 0.2044392523364486,
      "grad_norm": 1.1437208652496338,
      "learning_rate": 4.659267912772586e-05,
      "loss": 1.2893,
      "step": 350
    },
    {
      "epoch": 0.21904205607476634,
      "grad_norm": 1.6995948553085327,
      "learning_rate": 4.634929906542057e-05,
      "loss": 1.2931,
      "step": 375
    },
    {
      "epoch": 0.2336448598130841,
      "grad_norm": 1.3554176092147827,
      "learning_rate": 4.6105919003115266e-05,
      "loss": 1.2983,
      "step": 400
    },
    {
      "epoch": 0.24824766355140188,
      "grad_norm": 1.7259143590927124,
      "learning_rate": 4.586253894080997e-05,
      "loss": 1.2905,
      "step": 425
    },
    {
      "epoch": 0.2628504672897196,
      "grad_norm": 1.9354015588760376,
      "learning_rate": 4.561915887850468e-05,
      "loss": 1.2755,
      "step": 450
    },
    {
      "epoch": 0.2774532710280374,
      "grad_norm": 2.0533182621002197,
      "learning_rate": 4.5375778816199376e-05,
      "loss": 1.2834,
      "step": 475
    },
    {
      "epoch": 0.29205607476635514,
      "grad_norm": 1.8251081705093384,
      "learning_rate": 4.513239875389408e-05,
      "loss": 1.2942,
      "step": 500
    },
    {
      "epoch": 0.3066588785046729,
      "grad_norm": 2.004927635192871,
      "learning_rate": 4.488901869158879e-05,
      "loss": 1.3215,
      "step": 525
    },
    {
      "epoch": 0.3212616822429907,
      "grad_norm": 1.2310553789138794,
      "learning_rate": 4.464563862928349e-05,
      "loss": 1.2792,
      "step": 550
    },
    {
      "epoch": 0.3358644859813084,
      "grad_norm": 1.224444031715393,
      "learning_rate": 4.440225856697819e-05,
      "loss": 1.2837,
      "step": 575
    },
    {
      "epoch": 0.35046728971962615,
      "grad_norm": 1.1883083581924438,
      "learning_rate": 4.4158878504672904e-05,
      "loss": 1.2981,
      "step": 600
    },
    {
      "epoch": 0.36507009345794394,
      "grad_norm": 1.4960647821426392,
      "learning_rate": 4.39154984423676e-05,
      "loss": 1.2695,
      "step": 625
    },
    {
      "epoch": 0.3796728971962617,
      "grad_norm": 1.11279296875,
      "learning_rate": 4.367211838006231e-05,
      "loss": 1.2897,
      "step": 650
    },
    {
      "epoch": 0.3942757009345794,
      "grad_norm": 3.3723652362823486,
      "learning_rate": 4.3428738317757014e-05,
      "loss": 1.2812,
      "step": 675
    },
    {
      "epoch": 0.4088785046728972,
      "grad_norm": 1.3596960306167603,
      "learning_rate": 4.318535825545171e-05,
      "loss": 1.2809,
      "step": 700
    },
    {
      "epoch": 0.42348130841121495,
      "grad_norm": 1.7418222427368164,
      "learning_rate": 4.294197819314642e-05,
      "loss": 1.2778,
      "step": 725
    },
    {
      "epoch": 0.4380841121495327,
      "grad_norm": 1.2822264432907104,
      "learning_rate": 4.2698598130841124e-05,
      "loss": 1.2922,
      "step": 750
    },
    {
      "epoch": 0.4526869158878505,
      "grad_norm": 1.5854899883270264,
      "learning_rate": 4.245521806853583e-05,
      "loss": 1.2818,
      "step": 775
    },
    {
      "epoch": 0.4672897196261682,
      "grad_norm": 1.304915428161621,
      "learning_rate": 4.221183800623053e-05,
      "loss": 1.2858,
      "step": 800
    },
    {
      "epoch": 0.48189252336448596,
      "grad_norm": 2.4647057056427,
      "learning_rate": 4.1968457943925235e-05,
      "loss": 1.3063,
      "step": 825
    },
    {
      "epoch": 0.49649532710280375,
      "grad_norm": 1.6131970882415771,
      "learning_rate": 4.172507788161994e-05,
      "loss": 1.2998,
      "step": 850
    },
    {
      "epoch": 0.5110981308411215,
      "grad_norm": 1.4651964902877808,
      "learning_rate": 4.148169781931464e-05,
      "loss": 1.2727,
      "step": 875
    },
    {
      "epoch": 0.5257009345794392,
      "grad_norm": 1.4378418922424316,
      "learning_rate": 4.123831775700935e-05,
      "loss": 1.3033,
      "step": 900
    },
    {
      "epoch": 0.540303738317757,
      "grad_norm": 1.989951491355896,
      "learning_rate": 4.099493769470405e-05,
      "loss": 1.3088,
      "step": 925
    },
    {
      "epoch": 0.5549065420560748,
      "grad_norm": 1.9242433309555054,
      "learning_rate": 4.0751557632398756e-05,
      "loss": 1.2978,
      "step": 950
    },
    {
      "epoch": 0.5695093457943925,
      "grad_norm": 1.585597038269043,
      "learning_rate": 4.050817757009346e-05,
      "loss": 1.2688,
      "step": 975
    },
    {
      "epoch": 0.5841121495327103,
      "grad_norm": 1.4365991353988647,
      "learning_rate": 4.026479750778817e-05,
      "loss": 1.2686,
      "step": 1000
    },
    {
      "epoch": 0.5987149532710281,
      "grad_norm": 0.8879930973052979,
      "learning_rate": 4.0021417445482866e-05,
      "loss": 1.2766,
      "step": 1025
    },
    {
      "epoch": 0.6133177570093458,
      "grad_norm": 1.034176230430603,
      "learning_rate": 3.977803738317757e-05,
      "loss": 1.2892,
      "step": 1050
    },
    {
      "epoch": 0.6279205607476636,
      "grad_norm": 1.4151121377944946,
      "learning_rate": 3.953465732087228e-05,
      "loss": 1.2934,
      "step": 1075
    },
    {
      "epoch": 0.6425233644859814,
      "grad_norm": 1.232235312461853,
      "learning_rate": 3.9291277258566976e-05,
      "loss": 1.2865,
      "step": 1100
    },
    {
      "epoch": 0.657126168224299,
      "grad_norm": 1.1297640800476074,
      "learning_rate": 3.904789719626168e-05,
      "loss": 1.2917,
      "step": 1125
    },
    {
      "epoch": 0.6717289719626168,
      "grad_norm": 2.1133899688720703,
      "learning_rate": 3.880451713395639e-05,
      "loss": 1.2756,
      "step": 1150
    },
    {
      "epoch": 0.6863317757009346,
      "grad_norm": 1.190669298171997,
      "learning_rate": 3.856113707165109e-05,
      "loss": 1.2883,
      "step": 1175
    },
    {
      "epoch": 0.7009345794392523,
      "grad_norm": 1.7307356595993042,
      "learning_rate": 3.831775700934579e-05,
      "loss": 1.2593,
      "step": 1200
    },
    {
      "epoch": 0.7155373831775701,
      "grad_norm": 1.3430585861206055,
      "learning_rate": 3.8074376947040505e-05,
      "loss": 1.2574,
      "step": 1225
    },
    {
      "epoch": 0.7301401869158879,
      "grad_norm": 1.2042316198349,
      "learning_rate": 3.7830996884735204e-05,
      "loss": 1.2591,
      "step": 1250
    },
    {
      "epoch": 0.7447429906542056,
      "grad_norm": 1.240520715713501,
      "learning_rate": 3.758761682242991e-05,
      "loss": 1.2904,
      "step": 1275
    },
    {
      "epoch": 0.7593457943925234,
      "grad_norm": 1.49472177028656,
      "learning_rate": 3.7344236760124615e-05,
      "loss": 1.282,
      "step": 1300
    },
    {
      "epoch": 0.7739485981308412,
      "grad_norm": 0.9366068243980408,
      "learning_rate": 3.7100856697819314e-05,
      "loss": 1.2692,
      "step": 1325
    },
    {
      "epoch": 0.7885514018691588,
      "grad_norm": 1.3436166048049927,
      "learning_rate": 3.685747663551402e-05,
      "loss": 1.2745,
      "step": 1350
    },
    {
      "epoch": 0.8031542056074766,
      "grad_norm": 1.0009753704071045,
      "learning_rate": 3.6614096573208725e-05,
      "loss": 1.307,
      "step": 1375
    },
    {
      "epoch": 0.8177570093457944,
      "grad_norm": 0.9097902178764343,
      "learning_rate": 3.637071651090343e-05,
      "loss": 1.2801,
      "step": 1400
    },
    {
      "epoch": 0.8323598130841121,
      "grad_norm": 1.1544891595840454,
      "learning_rate": 3.612733644859813e-05,
      "loss": 1.2578,
      "step": 1425
    },
    {
      "epoch": 0.8469626168224299,
      "grad_norm": 1.488891839981079,
      "learning_rate": 3.5883956386292835e-05,
      "loss": 1.2637,
      "step": 1450
    },
    {
      "epoch": 0.8615654205607477,
      "grad_norm": 1.09141206741333,
      "learning_rate": 3.564057632398754e-05,
      "loss": 1.2838,
      "step": 1475
    },
    {
      "epoch": 0.8761682242990654,
      "grad_norm": 1.7092291116714478,
      "learning_rate": 3.5397196261682246e-05,
      "loss": 1.2853,
      "step": 1500
    },
    {
      "epoch": 0.8907710280373832,
      "grad_norm": 1.429503083229065,
      "learning_rate": 3.515381619937695e-05,
      "loss": 1.2619,
      "step": 1525
    },
    {
      "epoch": 0.905373831775701,
      "grad_norm": 0.7936446070671082,
      "learning_rate": 3.491043613707165e-05,
      "loss": 1.2852,
      "step": 1550
    },
    {
      "epoch": 0.9199766355140186,
      "grad_norm": 1.4954617023468018,
      "learning_rate": 3.466705607476636e-05,
      "loss": 1.2656,
      "step": 1575
    },
    {
      "epoch": 0.9345794392523364,
      "grad_norm": 1.0913172960281372,
      "learning_rate": 3.442367601246106e-05,
      "loss": 1.2761,
      "step": 1600
    },
    {
      "epoch": 0.9491822429906542,
      "grad_norm": 1.497362494468689,
      "learning_rate": 3.418029595015577e-05,
      "loss": 1.2578,
      "step": 1625
    },
    {
      "epoch": 0.9637850467289719,
      "grad_norm": 1.2359390258789062,
      "learning_rate": 3.393691588785047e-05,
      "loss": 1.2823,
      "step": 1650
    },
    {
      "epoch": 0.9783878504672897,
      "grad_norm": 1.3532193899154663,
      "learning_rate": 3.369353582554517e-05,
      "loss": 1.2507,
      "step": 1675
    },
    {
      "epoch": 0.9929906542056075,
      "grad_norm": 1.5660265684127808,
      "learning_rate": 3.345015576323988e-05,
      "loss": 1.2303,
      "step": 1700
    },
    {
      "epoch": 1.0075934579439252,
      "grad_norm": 1.0869373083114624,
      "learning_rate": 3.3206775700934584e-05,
      "loss": 1.2654,
      "step": 1725
    },
    {
      "epoch": 1.022196261682243,
      "grad_norm": 3.2839553356170654,
      "learning_rate": 3.296339563862928e-05,
      "loss": 1.2373,
      "step": 1750
    },
    {
      "epoch": 1.0367990654205608,
      "grad_norm": 1.678070068359375,
      "learning_rate": 3.272001557632399e-05,
      "loss": 1.2514,
      "step": 1775
    },
    {
      "epoch": 1.0514018691588785,
      "grad_norm": 1.4856014251708984,
      "learning_rate": 3.2476635514018694e-05,
      "loss": 1.2536,
      "step": 1800
    },
    {
      "epoch": 1.0660046728971964,
      "grad_norm": 1.4253722429275513,
      "learning_rate": 3.223325545171339e-05,
      "loss": 1.2425,
      "step": 1825
    },
    {
      "epoch": 1.080607476635514,
      "grad_norm": 2.46571683883667,
      "learning_rate": 3.1989875389408105e-05,
      "loss": 1.2529,
      "step": 1850
    },
    {
      "epoch": 1.0952102803738317,
      "grad_norm": 1.9963330030441284,
      "learning_rate": 3.1746495327102804e-05,
      "loss": 1.2308,
      "step": 1875
    },
    {
      "epoch": 1.1098130841121496,
      "grad_norm": 1.8880510330200195,
      "learning_rate": 3.150311526479751e-05,
      "loss": 1.2522,
      "step": 1900
    },
    {
      "epoch": 1.1244158878504673,
      "grad_norm": 1.6337417364120483,
      "learning_rate": 3.1259735202492215e-05,
      "loss": 1.2283,
      "step": 1925
    },
    {
      "epoch": 1.139018691588785,
      "grad_norm": 1.1964950561523438,
      "learning_rate": 3.101635514018692e-05,
      "loss": 1.2609,
      "step": 1950
    },
    {
      "epoch": 1.153621495327103,
      "grad_norm": 2.3081235885620117,
      "learning_rate": 3.077297507788162e-05,
      "loss": 1.2268,
      "step": 1975
    },
    {
      "epoch": 1.1682242990654206,
      "grad_norm": 1.3303532600402832,
      "learning_rate": 3.0529595015576326e-05,
      "loss": 1.2321,
      "step": 2000
    },
    {
      "epoch": 1.1828271028037383,
      "grad_norm": 1.8234593868255615,
      "learning_rate": 3.028621495327103e-05,
      "loss": 1.205,
      "step": 2025
    },
    {
      "epoch": 1.1974299065420562,
      "grad_norm": 1.8765895366668701,
      "learning_rate": 3.0042834890965733e-05,
      "loss": 1.2416,
      "step": 2050
    },
    {
      "epoch": 1.2120327102803738,
      "grad_norm": 1.4659836292266846,
      "learning_rate": 2.979945482866044e-05,
      "loss": 1.2031,
      "step": 2075
    },
    {
      "epoch": 1.2266355140186915,
      "grad_norm": 1.5853352546691895,
      "learning_rate": 2.955607476635514e-05,
      "loss": 1.2455,
      "step": 2100
    },
    {
      "epoch": 1.2412383177570094,
      "grad_norm": 2.147944450378418,
      "learning_rate": 2.9312694704049847e-05,
      "loss": 1.2289,
      "step": 2125
    },
    {
      "epoch": 1.2558411214953271,
      "grad_norm": 1.408110499382019,
      "learning_rate": 2.906931464174455e-05,
      "loss": 1.2279,
      "step": 2150
    },
    {
      "epoch": 1.2704439252336448,
      "grad_norm": 5.482025623321533,
      "learning_rate": 2.882593457943925e-05,
      "loss": 1.2022,
      "step": 2175
    },
    {
      "epoch": 1.2850467289719627,
      "grad_norm": 2.2242343425750732,
      "learning_rate": 2.8582554517133957e-05,
      "loss": 1.1926,
      "step": 2200
    },
    {
      "epoch": 1.2996495327102804,
      "grad_norm": 1.2908236980438232,
      "learning_rate": 2.833917445482866e-05,
      "loss": 1.2444,
      "step": 2225
    },
    {
      "epoch": 1.314252336448598,
      "grad_norm": 1.7159266471862793,
      "learning_rate": 2.809579439252337e-05,
      "loss": 1.2402,
      "step": 2250
    },
    {
      "epoch": 1.3288551401869158,
      "grad_norm": 1.5834039449691772,
      "learning_rate": 2.7852414330218067e-05,
      "loss": 1.2647,
      "step": 2275
    },
    {
      "epoch": 1.3434579439252337,
      "grad_norm": 2.3847553730010986,
      "learning_rate": 2.7609034267912776e-05,
      "loss": 1.1963,
      "step": 2300
    },
    {
      "epoch": 1.3580607476635513,
      "grad_norm": 2.512308120727539,
      "learning_rate": 2.736565420560748e-05,
      "loss": 1.1664,
      "step": 2325
    },
    {
      "epoch": 1.3726635514018692,
      "grad_norm": 1.458910584449768,
      "learning_rate": 2.7122274143302184e-05,
      "loss": 1.1853,
      "step": 2350
    },
    {
      "epoch": 1.387266355140187,
      "grad_norm": 1.5330668687820435,
      "learning_rate": 2.6878894080996887e-05,
      "loss": 1.2232,
      "step": 2375
    },
    {
      "epoch": 1.4018691588785046,
      "grad_norm": 1.806384563446045,
      "learning_rate": 2.663551401869159e-05,
      "loss": 1.2199,
      "step": 2400
    },
    {
      "epoch": 1.4164719626168225,
      "grad_norm": 1.308348298072815,
      "learning_rate": 2.6392133956386295e-05,
      "loss": 1.2218,
      "step": 2425
    },
    {
      "epoch": 1.4310747663551402,
      "grad_norm": 2.6495676040649414,
      "learning_rate": 2.6148753894080997e-05,
      "loss": 1.1978,
      "step": 2450
    },
    {
      "epoch": 1.4456775700934579,
      "grad_norm": 3.4317822456359863,
      "learning_rate": 2.5905373831775702e-05,
      "loss": 1.1816,
      "step": 2475
    },
    {
      "epoch": 1.4602803738317758,
      "grad_norm": 1.3193268775939941,
      "learning_rate": 2.5661993769470405e-05,
      "loss": 1.2347,
      "step": 2500
    },
    {
      "epoch": 1.4748831775700935,
      "grad_norm": 4.286243438720703,
      "learning_rate": 2.5418613707165114e-05,
      "loss": 1.1917,
      "step": 2525
    },
    {
      "epoch": 1.4894859813084111,
      "grad_norm": 2.081172227859497,
      "learning_rate": 2.5175233644859813e-05,
      "loss": 1.1687,
      "step": 2550
    },
    {
      "epoch": 1.5040887850467288,
      "grad_norm": 2.174849271774292,
      "learning_rate": 2.4931853582554518e-05,
      "loss": 1.1987,
      "step": 2575
    },
    {
      "epoch": 1.5186915887850467,
      "grad_norm": 2.3547253608703613,
      "learning_rate": 2.4688473520249224e-05,
      "loss": 1.2039,
      "step": 2600
    },
    {
      "epoch": 1.5332943925233646,
      "grad_norm": 2.193504571914673,
      "learning_rate": 2.4445093457943926e-05,
      "loss": 1.214,
      "step": 2625
    },
    {
      "epoch": 1.5478971962616823,
      "grad_norm": 2.0951077938079834,
      "learning_rate": 2.4201713395638632e-05,
      "loss": 1.1964,
      "step": 2650
    },
    {
      "epoch": 1.5625,
      "grad_norm": 4.151066303253174,
      "learning_rate": 2.3958333333333334e-05,
      "loss": 1.2112,
      "step": 2675
    },
    {
      "epoch": 1.5771028037383177,
      "grad_norm": 1.7908295392990112,
      "learning_rate": 2.371495327102804e-05,
      "loss": 1.2166,
      "step": 2700
    },
    {
      "epoch": 1.5917056074766354,
      "grad_norm": 1.7154693603515625,
      "learning_rate": 2.3471573208722742e-05,
      "loss": 1.2189,
      "step": 2725
    },
    {
      "epoch": 1.6063084112149533,
      "grad_norm": 3.3879358768463135,
      "learning_rate": 2.3228193146417444e-05,
      "loss": 1.2052,
      "step": 2750
    },
    {
      "epoch": 1.6209112149532712,
      "grad_norm": 2.029980421066284,
      "learning_rate": 2.298481308411215e-05,
      "loss": 1.2546,
      "step": 2775
    },
    {
      "epoch": 1.6355140186915889,
      "grad_norm": 1.3046573400497437,
      "learning_rate": 2.2741433021806856e-05,
      "loss": 1.203,
      "step": 2800
    },
    {
      "epoch": 1.6501168224299065,
      "grad_norm": 1.6284373998641968,
      "learning_rate": 2.2498052959501558e-05,
      "loss": 1.2088,
      "step": 2825
    },
    {
      "epoch": 1.6647196261682242,
      "grad_norm": 2.300353527069092,
      "learning_rate": 2.2254672897196263e-05,
      "loss": 1.1788,
      "step": 2850
    },
    {
      "epoch": 1.679322429906542,
      "grad_norm": 2.9716975688934326,
      "learning_rate": 2.201129283489097e-05,
      "loss": 1.2332,
      "step": 2875
    },
    {
      "epoch": 1.6939252336448598,
      "grad_norm": 1.9122971296310425,
      "learning_rate": 2.176791277258567e-05,
      "loss": 1.2194,
      "step": 2900
    },
    {
      "epoch": 1.7085280373831777,
      "grad_norm": 2.4561092853546143,
      "learning_rate": 2.1524532710280377e-05,
      "loss": 1.2132,
      "step": 2925
    },
    {
      "epoch": 1.7231308411214954,
      "grad_norm": 1.614791989326477,
      "learning_rate": 2.128115264797508e-05,
      "loss": 1.2181,
      "step": 2950
    },
    {
      "epoch": 1.737733644859813,
      "grad_norm": 2.128077983856201,
      "learning_rate": 2.103777258566978e-05,
      "loss": 1.2176,
      "step": 2975
    },
    {
      "epoch": 1.7523364485981308,
      "grad_norm": 1.6871880292892456,
      "learning_rate": 2.0794392523364487e-05,
      "loss": 1.2084,
      "step": 3000
    }
  ],
  "logging_steps": 25,
  "max_steps": 5136,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 3,
  "save_steps": 500,
  "total_flos": 1.2890320474546176e+16,
  "train_batch_size": 32,
  "trial_name": null,
  "trial_params": null
}
