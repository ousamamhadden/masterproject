Loaded module: python3/3.6.2
Loaded dependency [numpy/1.13.1-python-3.6.2-openblas-0.2.20]: openblas/0.2.20
Loaded module: numpy/1.13.1-python-3.6.2-openblas-0.2.20

Loading numpy/1.13.1-python-3.6.2-openblas-0.2.20
  Loading requirement: openblas/0.2.20
Loaded module: scipy/0.19.1-python-3.6.2
Loaded module: matplotlib/2.0.2-python-3.6.2
Loaded module: cuda/11.6
Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Tokenizing data:   0%|          | 0/76650 [00:00<?, ? examples/s]Tokenizing data:   1%|▏         | 1000/76650 [00:04<05:48, 217.02 examples/s]Tokenizing data:   3%|▎         | 2000/76650 [00:06<03:36, 344.99 examples/s]Tokenizing data:   4%|▍         | 3000/76650 [00:07<02:35, 475.11 examples/s]Tokenizing data:   5%|▌         | 4000/76650 [00:08<01:53, 642.28 examples/s]Tokenizing data:   7%|▋         | 5000/76650 [00:08<01:26, 830.49 examples/s]Tokenizing data:   8%|▊         | 6000/76650 [00:09<01:09, 1013.69 examples/s]Tokenizing data:   9%|▉         | 7000/76650 [00:09<00:59, 1174.45 examples/s]Tokenizing data:  10%|█         | 8000/76650 [00:10<00:51, 1335.07 examples/s]Tokenizing data:  12%|█▏        | 9000/76650 [00:10<00:45, 1473.19 examples/s]Tokenizing data:  13%|█▎        | 10000/76650 [00:11<00:41, 1592.08 examples/s]Tokenizing data:  14%|█▍        | 11000/76650 [00:12<00:39, 1670.79 examples/s]Tokenizing data:  16%|█▌        | 12000/76650 [00:12<00:37, 1741.39 examples/s]Tokenizing data:  17%|█▋        | 13000/76650 [00:13<00:35, 1777.73 examples/s]Tokenizing data:  18%|█▊        | 14000/76650 [00:13<00:34, 1821.53 examples/s]Tokenizing data:  20%|█▉        | 15000/76650 [00:14<00:33, 1846.85 examples/s]Tokenizing data:  21%|██        | 16000/76650 [00:14<00:32, 1893.01 examples/s]Tokenizing data:  22%|██▏       | 17000/76650 [00:15<00:31, 1903.70 examples/s]Tokenizing data:  23%|██▎       | 18000/76650 [00:15<00:30, 1920.87 examples/s]Tokenizing data:  25%|██▍       | 19000/76650 [00:16<00:29, 1927.68 examples/s]Tokenizing data:  26%|██▌       | 20000/76650 [00:16<00:29, 1950.44 examples/s]Tokenizing data:  27%|██▋       | 21000/76650 [00:17<00:28, 1947.27 examples/s]Tokenizing data:  29%|██▊       | 22000/76650 [00:17<00:27, 1967.13 examples/s]Tokenizing data:  30%|███       | 23000/76650 [00:18<00:30, 1766.30 examples/s]Tokenizing data:  31%|███▏      | 24000/76650 [00:18<00:28, 1817.68 examples/s]Tokenizing data:  33%|███▎      | 25000/76650 [00:19<00:27, 1847.46 examples/s]Tokenizing data:  34%|███▍      | 26000/76650 [00:19<00:27, 1872.20 examples/s]Tokenizing data:  35%|███▌      | 27000/76650 [00:20<00:26, 1887.29 examples/s]Tokenizing data:  37%|███▋      | 28000/76650 [00:20<00:25, 1901.92 examples/s]Tokenizing data:  38%|███▊      | 29000/76650 [00:21<00:24, 1912.95 examples/s]Tokenizing data:  39%|███▉      | 30000/76650 [00:21<00:24, 1911.95 examples/s]Tokenizing data:  40%|████      | 31000/76650 [00:22<00:24, 1899.21 examples/s]Tokenizing data:  42%|████▏     | 32000/76650 [00:23<00:23, 1888.69 examples/s]Tokenizing data:  43%|████▎     | 33000/76650 [00:23<00:23, 1887.53 examples/s]Tokenizing data:  44%|████▍     | 34000/76650 [00:24<00:22, 1887.83 examples/s]Tokenizing data:  46%|████▌     | 35000/76650 [00:24<00:21, 1898.80 examples/s]